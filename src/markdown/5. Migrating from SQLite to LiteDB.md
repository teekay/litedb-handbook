# Migrating from SQLite to LiteDB

By now, you should have a good overview of LiteDB capabilities and how it stacks compared to SQLite.

This chapter is for you if you have an existing .NET application built atop SQLite and want to migrate the persistence layer to LiteDB.

## When to migrate

First, consider how your data is structured.

Relational data can be remapped to documents with varying degrees of effort. The more normalized your database is, the more work you will have to invest in it.

The easier it is to export your data as JSON documents, the more likely your data can be migrated while keeping its integrity intact.

Next, review how you query your database.

Do you leverage advanced SQL features, e.g., for analytical purposes, or do you just hydrate your models and maintain your application logic in code? In the latter case, you using SQLite only as a data store, and you can swap it for LiteDB easily.

How about performance - how critical is it to you? As we have seen, LiteDB can be as much as 2x slower than SQLite (10x in one unfortunate test case), but it may not matter much since read queries typically return in 10-50 ms.

To sum up: you may see a green light to migrate when your data is already structured such that it can be expressed as documents without any information loss, when you use SQLite primarily as a data store and not an analytical engine, and when query performance is just one factor out of many.

## When not to migrate

Your data might dictate that you should not migrate.

A higher level of normalization, accompanied by the usage of foreign key constraints, unique indexes, etc., could might the migration counter-productive. If you were to do it (and keep the constraints), you would have to replicate the features that a SQL database gives you for free in your code.

How you query and manipulate your data is another determining factor.

LiteDB has a SQL-like API but comes nowhere close to matching the full SQL standard. If you leveraging SQL features that LiteDB does not have (let alone [extensions](https://github.com/nalgeon/sqlean/)), you may not want to migrate. You can express a very complex logical operation in a few lines of SQL that would take up the entire screen space (or more) in C#.

LiteDB performs very well, but as weâ€™ve seen before, SQLite is almost always faster. There are applications where speed concerns are paramount. If yours counts among them, consider sticking with SQLite.

To sum up: when you are using the full potential of SQLite, or your data is naturally relational, or query performance is the limiting factor in your application, you should think twice before attempting the migration.

## Preparing your code

Whether or not you have to do anything in your code depends on how tightly integrated you are with SQLite.

In the best-case scenario, your data access layer completely encapsulates SQLite, and the rest of your application is blissfully unaware of how your data is persisted. You are free to implement it as you see fit without any regression risks. 

If thatâ€™s not the case and you make SQLite-specific function calls from multiple places in your app, you will want to address that first.

The goal here would not be to replace SQLite queries with LiteDB queries (although you can) but to isolate and abstract away your data access code. Thatâ€™s generally the best practice anyway.

## Preparing your data

The way you migrate depends on how your app is deployed.

If itâ€™s sitting on your local PC or in the cloud and thereâ€™s exactly one instance of the database, you can migrate manually and switch over to the new database in a single release.

For apps that are deployed across a wide variety of clients, e.g., desktop apps, youâ€™re going to have to program a migration, release a version that includes both clients (SQLite and LiteDB), run the migration, and finally drop the SQLite client in the following version(s).

How you execute the migration itself, however, is under your control.

You could do it in the application code: hydrate the SQLite models, convert them to LiteDB models, and populate the LiteDB database.

I favor a database-centric approach that limits the new lines of code I have to write as close to zero as I can get.

You can take advantage of the following:

- SQLite has built-in JSON and aggregations functions
- LiteDB can import documents from an arbitrary JSON

If you can make SQLite spit out a JSON representation of your data, you are good to go!

### Let the database do the migration

Yes, itâ€™s true: with a little SQL, you can export your SQLite tables to JSON.

Try this yourself: run the SQLite command-line client, type `.mode json`, then run `select * from mytable` (replacing `mytable` with whatever table comes to mind).

Please note that this is the feature of the SQLite client, not the SQLite database engine itself. We want to program a migration that can scale from 1 computer to thousands and run unsupervised, therefore we need to do a little more work.

To make the process easier, you may want to create views if your data is normalized such that it maps to the final document that you will feed to LiteDB. You can of course export data from views as you can from tables.

In our case, we will largely map the SQLite tables to LiteDB collections without changes.

There are a few data type concerns that we must address, however.

LiteDB uses [extended JSON](https://github.com/mongodb/specifications/blob/master/source/extended-json.rst) to store the [following data types](https://www.litedb.org/docs/data-structure/):

- long (Int64)
- decimal
- Date
- blob/binary
- and a few others like ObjectId

SQLite has just one numeric type for integers, and if you know that a particular field can hold Int64 values, you must export it in the LiteDB format: `"field": { "$numberLong": "15654685132122222564"}`. Similarly, for the other types (see docs).

### Example: migrating the music library

Letâ€™s migrate the music library.

Recall that our data model has two major and one helper entities: Track, Playlist, and an entity that links tracks to playlists.

Examining the Track entity, we find that we have four fields that can hold Int64 values: `StartTime`, `EndTime`, `LastScannedOn`, and `Duration`. These store `DateTime` or  `TimeSpan` values encoded as â€œticksâ€.

The property `ConfirmedReadable` holds a boolean, but since SQLite does not have booleans, the actual value stored is 0 or 1.

The first obstacle is the property `WaveformData`. It is a byte array representing the trackâ€™s waveform, and SQLite stores it as a blob.

Blobs cannot be represented in JSON directly, and unfortunately, SQLite and LiteDB chose a different representation: SQLite can export blobs in the [hexadecimal representation](https://sqlite.org/lang_corefunc.html#hex) while LiteDB stores them base64 encoded.

Therefore, if you store blobs in your tables, SQLite will export them in hex, and we will need to add an additional step to convert them to base64.

The rest is easy.

Weâ€™ll ask SQLite to build the JSON using the function `json_object`, which takes an arbitrary number of arguments: a key name followed by the value. Note that weâ€™re using the extended JSON format for the data types mentioned above.

To convert 1 or 0 to booleans, we can use this expression: `iif(t.ConfirmedReadable=1, json('true'), json('false'))`.

The final export SQL script looks like this:

```sql
with tracks_with_waveform as (
  select json_object('_id', t.Id, 
                    'Uri', t.Filepath, 
                    'Title', t.Title, 
                    'Artist', t.Artist, 
                    'AlbumArtist', t.AlbumArtist, 
                    'Conductor', t.Conductor, 
                    'Album', t.Album, 
                    'Genre', t.Genre, 
                    'Year', t.Year, 
                    'Duration', json_object('$numberLong', t.Duration),
                    'Comment', t.Comment, 
                    'Grouping', t.Grouping, 
                    'BPM', t.BPM, 
                    'ReplayGain', t.ReplayGain, 
                    'Rating', t.Rating, 
                    'StartTime', json_object('$numberLong', t.StartTime), 
                    'EndTime', json_object('$numberLong', t.EndTime), 
                    'LastScannedOn', json_object('$numberLong', t.LastScannedOn),               
                    'ConfirmedReadable', iif(t.ConfirmedReadable=1, json('true'), json('false')),
                    'SearchIndex', t.SearchIndex,
                    'WaveformData', json_object('$binary', hex(t.WaveformData))) as json 
  from Track as t 
  where t.WaveformData is not null),
tracks_without_waveform as (
  select json_object('_id', t.Id, 
                    'Uri', t.Filepath, 
                    'Title', t.Title, 
                    'Artist', t.Artist, 
                    'AlbumArtist', t.AlbumArtist, 
                    'Conductor', t.Conductor, 
                    'Album', t.Album, 
                    'Genre', t.Genre, 
                    'Year', t.Year, 
                    'Duration', json_object('$numberLong', t.Duration),
                    'Comment', t.Comment, 
                    'Grouping', t.Grouping, 
                    'BPM', t.BPM, 
                    'ReplayGain', t.ReplayGain, 
                    'Rating', t.Rating, 
                    'StartTime', json_object('$numberLong', t.StartTime), 
                    'EndTime', json_object('$numberLong', t.EndTime), 
                    'LastScannedOn', json_object('$numberLong', t.LastScannedOn),               
                    'ConfirmedReadable', iif(t.ConfirmedReadable=1, json('true'), json('false')),
                    'SearchIndex', t.SearchIndex) as json
  from Track as t
  where t.WaveformData is null),
all_tracks as (
 select json from tracks_with_waveform
 union all 
 select json from tracks_without_waveform)
select '[' || group_concat(json) || ']' from all_tracks;
```

Since the output JSON contains binary data encoded as hexadecimal strings, weâ€™ll need to run the following step to convert those into the base64 format.

For example, this is how we could accomplish it using PowerShell:

```powershell
$tracks = Get-Content -Raw /path-to-json | ConvertFrom-Json
$tracks | Where WaveformData -ne $null | ForEach-Object { $_.WaveformData.'$binary' = [System.Convert]::ToBase64String([System.Convert]::FromHexString($_.WaveformData.'$binary')) }
$tracks | ConvertTo-Json | Out-File -FilePath /path-to-output-json
```

Then you can take the final JSON and load it into your LiteDB database:

```sql
select $ into tracks:int from $file("/path/to/json");
```

The playlist model is a lot simpler, hence the query will be, too:

```sql
with playlists as (
  select json_object('_id', p.Id, 
                     'Uri', p.Filename,
                     'Comment', p.Comment) as json 
  from Playlist p) 
select '[' || group_concat(json) || ']' from playlists;
```

Then load the JSON into LiteDB:

```sql
select $ into playlists:int from $file("/path/to/json")
```

Finally, letâ€™s export the mapping between tracks and playlists.

Recall that in SQLite, itâ€™s implemented as a many-to-many entity with fields referencing the track and playlist tables. In LiteDB, we want to take advantage of collection references instead so that we can fetch a list of tracks from a particular playlist with a single query.

The export SQL:

```sql
with map as (
  select json_object('PlaylistId', m.PlaylistId, 
                     'Track', json_object('$id', m.TrackId, '$ref', 'tracks'), 
                     'Position', m.Position, 
                     'CreatedAt', json_object('$numberLong', m.CreatedAt))
  as json from PlaylistTracks as m)
select '[' || group_concat(json) || ']' from map;
```

> ðŸ’¡ We have not included a reference to the `playlists` collection in the above schema, but we could. That way, you could obtain both the full playlist information as well as details of all the tracks inside the playlist. The results set would be a lot fatter, though, as the same playlist details would be included with each track.

Load it into LiteDB:

```sql
select $ into playlist_tracks:int from $file("/path/to/json")
```

Confirm that you can query the tracks from a particular playlist:

```sql
SELECT $ FROM playlist_tracks INCLUDE Track WHERE PlaylistId=123;
```

Thatâ€™s it!

Now that we have proved we can export data from SQLite and import it into LiteDB in a lossless fashion, letâ€™s look at how to automate this. After all, if your app is distributed and every user has their own database, they will need help. Ideally, they wonâ€™t even know you have changed the database engine ðŸ˜„

## Running the migration

Now that you have figured out how to program the migration for your app, youâ€™ll need to think about how to run the migration for your users.

We will focus on desktop scenarios. However, since weâ€™ll want to run the migration unattended, the steps we design might apply to networked deployments.

### When and how to run the migration

The best time to do this is before the user starts interacting with the app.

Given that your current version is, for example, 2.17.5, and the new version that contains the database change is 3.0, either the user downloads an installer, or your app has a built-in updater that does this on their behalf.

When the new version is installed, either your own application code handles the migration or you run a one-time migration script that executes after the updated binary has been installed.

As there are many ways to distribute and update .NET apps, we canâ€™t get into the specifics. For example, I have used [Squirrel.Windows](https://github.com/Squirrel/Squirrel.Windows) and did not find a way to hook into the update process to run a custom code like a database migration.

When I donâ€™t get any support from the installer, I would run any migrations as a custom step when the application starts that runs before everything else.

The steps you would take include:

1. Export data from SQLite as I described it in the previous section
2. Run any transformations on the exported JSON if needed (e.g., convert the binary data from hex to base64)
3. Import data into LiteDB
4. (Optionally) perform any clean-up, such as deleting the SQLite database file

There may be complicating factors. For example, the migration may fail for some users. I would include support for both databases for some time until bug reports associated with the migration stop coming in.

This is why I recommended abstracting the data access layer. If the migration fails, your users can still use the app with the old database until you fix the issue in the next release.

\newpage
